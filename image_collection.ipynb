{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5dc9b2-b6db-4b2d-bc0b-6ea8dfaa7952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary packages/dependencies\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Initialize MediaPipe modules for holistic model and drawing utilities\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load labels from the file\n",
    "def load_labels():\n",
    "    with open(\"labels.txt\", \"r\") as file:\n",
    "        labels = [line.strip() for line in file.readlines()]\n",
    "    return {idx: label for idx, label in enumerate(labels)}\n",
    "\n",
    "labels = load_labels()\n",
    "\n",
    "# Create dataset directory\n",
    "DATA_PATH = os.path.join('dataset')\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "# Function to make detection\n",
    "def mediapipe_detection(frame, model):\n",
    "    try:\n",
    "        # Color conversion BGR to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Making image non-writable\n",
    "        image.flags.writeable = False\n",
    "        # Make prediction\n",
    "        results = model.process(image)\n",
    "        # Making image writable\n",
    "        image.flags.writeable = True\n",
    "        # Color conversion RGB to BGR\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        return image, results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in mediapipe_detection: {e}\")\n",
    "        return frame, None\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    try:\n",
    "        # Draw face connections \n",
    "        if results.face_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                                      mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),  \n",
    "                                      mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "        # Draw pose connections\n",
    "        if results.pose_landmarks: \n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                      mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                                      mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
    "        # Draw left hand connections \n",
    "        if results.left_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,  \n",
    "                                      mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                                      mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2))\n",
    "        # Draw right hand connections\n",
    "        if results.right_hand_landmarks:   \n",
    "            mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                      mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                                      mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in draw_styled_landmarks: {e}\")\n",
    "\n",
    "# Function to extract keypoints\n",
    "def extract_keypoints(results):\n",
    "    try:\n",
    "        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "        face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "        left_hand = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "        right_hand = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "        return np.concatenate([pose, face, left_hand, right_hand])\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_keypoints: {e}\")\n",
    "        return np.zeros(33 * 4 + 468 * 3 + 21 * 3 + 21 * 3)\n",
    "\n",
    "# Function to capture gestures\n",
    "def capture_gestures(label, model, num_samples=100, num_frames=30):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    for sample in range(num_samples):\n",
    "        for frame_num in range(num_frames):\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                continue\n",
    "            \n",
    "            image, results = mediapipe_detection(frame, model)\n",
    "            \n",
    "            # Draw landmarks on the image\n",
    "            draw_styled_landmarks(image, results)\n",
    "    \n",
    "            # Wait for 1 second after each sample\n",
    "            if frame_num == 0:\n",
    "                cv2.putText(image, 'Starting collection', (120, 200),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                cv2.putText(image, 'Collecting frames for {} video number {}'.format(label, sample), (15, 12),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                cv2.waitKey(1000)\n",
    "            else:\n",
    "                cv2.putText(image, 'Collecting frames for {} video number {}'.format(label, sample), (15, 12),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "            keypoints = extract_keypoints(results)\n",
    "\n",
    "            try:\n",
    "                os.makedirs(os.path.join(DATA_PATH, label, str(sample)), exist_ok=True)\n",
    "                frame_path = os.path.join(DATA_PATH, label, str(sample), str(frame_num))\n",
    "                np.save(frame_path, keypoints)\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving keypoints: {e}\")\n",
    "            \n",
    "            cv2.imshow('MediaPipe Detection', image)\n",
    "            \n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    try:\n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "            for label in labels.keys():\n",
    "                capture_gestures(label, holistic, num_samples=100, num_frames=30)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sign_language",
   "language": "python",
   "name": "sign_language"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
